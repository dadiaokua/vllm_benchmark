# Ubuntuä¸Šç¼–è¯‘å’Œè¿è¡ŒvLLMåŸºå‡†æµ‹è¯•æŒ‡å—

## ğŸ–¥ï¸ ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Ubuntu 18.04+ (æ¨è Ubuntu 20.04/22.04)
- **GPU**: NVIDIA GPU (æ”¯æŒCUDA 11.8+)
- **å†…å­˜**: è‡³å°‘16GB RAM (æ¨è32GB+)
- **å­˜å‚¨**: è‡³å°‘50GBå¯ç”¨ç©ºé—´

## ğŸ“¦ å®‰è£…ä¾èµ–

### 1. æ›´æ–°ç³»ç»ŸåŒ…
```bash
sudo apt update && sudo apt upgrade -y
```

### 2. å®‰è£…åŸºç¡€å·¥å…·
```bash
sudo apt install -y \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    vim \
    htop \
    tmux \
    tree
```

### 3. å®‰è£…Python 3.9+
```bash
# å®‰è£…Python 3.9
sudo apt install -y python3.9 python3.9-dev python3.9-venv python3-pip

# è®¾ç½®é»˜è®¤Pythonç‰ˆæœ¬
sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1
sudo update-alternatives --install /usr/bin/pip3 pip3 /usr/bin/pip3 1
```

### 4. å®‰è£…CUDA (å¦‚æœéœ€è¦GPUæ”¯æŒ)
```bash
# ä¸‹è½½CUDA 12.1 (æˆ–æœ€æ–°ç‰ˆæœ¬)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt update
sudo apt install -y cuda-toolkit-12-1

# æ·»åŠ CUDAåˆ°PATH
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

## ğŸš€ ç¼–è¯‘å’Œå®‰è£…vLLM

### 1. å…‹éš†ä»£ç ä»“åº“
```bash
# å…‹éš†ä½ çš„åŸºå‡†æµ‹è¯•é¡¹ç›®
git clone <your-repo-url> vllm-benchmark
cd vllm-benchmark
```

### 2. åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv vllm_env

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source vllm_env/bin/activate

# å‡çº§pip
pip install --upgrade pip setuptools wheel
```

### 3. å®‰è£…PyTorch
```bash
# å®‰è£…PyTorch (CUDAç‰ˆæœ¬)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# éªŒè¯PyTorch CUDAæ”¯æŒ
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, GPU count: {torch.cuda.device_count()}')"
```

### 4. å®‰è£…vLLM
```bash
# æ–¹æ³•1: ä»PyPIå®‰è£… (æ¨è)
pip install vllm

# æ–¹æ³•2: ä»æºç ç¼–è¯‘ (å¦‚æœéœ€è¦æœ€æ–°åŠŸèƒ½)
# git clone https://github.com/vllm-project/vllm.git
# cd vllm
# pip install -e .
# cd ..
```

### 5. å®‰è£…å…¶ä»–ä¾èµ–
```bash
# å®‰è£…åŸºå‡†æµ‹è¯•æ‰€éœ€çš„ä¾èµ–
pip install -r requirements.txt

# å¦‚æœæ²¡æœ‰requirements.txtï¼Œæ‰‹åŠ¨å®‰è£…å¸¸ç”¨ä¾èµ–
pip install \
    transformers \
    accelerate \
    datasets \
    numpy \
    pandas \
    matplotlib \
    seaborn \
    tqdm \
    requests \
    openai \
    aiohttp \
    asyncio \
    psutil
```

## ğŸ“ é¡¹ç›®ç»“æ„è®¾ç½®

### 1. åˆ›å»ºå¿…è¦çš„ç›®å½•
```bash
# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p log

# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p models

# åˆ›å»ºç»“æœç›®å½•
mkdir -p results

# åˆ›å»ºpromptæ•°æ®ç›®å½•
mkdir -p prompt_hub
```

### 2. ä¸‹è½½æµ‹è¯•æ•°æ®
```bash
# åˆ›å»ºç¤ºä¾‹promptæ–‡ä»¶
cat > prompt_hub/short_prompts.json << 'EOF'
[
    {"prompt": "What is the capital of France?", "max_tokens": 50},
    {"prompt": "Explain quantum computing in simple terms.", "max_tokens": 100},
    {"prompt": "Write a short poem about spring.", "max_tokens": 80}
]
EOF

cat > prompt_hub/long_prompts.json << 'EOF'
[
    {"prompt": "Write a detailed essay about the impact of artificial intelligence on modern society, covering both benefits and challenges.", "max_tokens": 500},
    {"prompt": "Explain the process of photosynthesis in plants, including the chemical reactions and environmental factors involved.", "max_tokens": 400}
]
EOF
```

## ğŸ”§ é…ç½®æ–‡ä»¶è®¾ç½®

### 1. åˆ›å»ºé…ç½®æ–‡ä»¶
```bash
# åˆ›å»ºé…ç½®ç›®å½•
mkdir -p config

# åˆ›å»ºåŸºç¡€é…ç½®æ–‡ä»¶
cat > config/Config.py << 'EOF'
#!/usr/bin/env python3
"""
å…¨å±€é…ç½®æ–‡ä»¶
"""

GLOBAL_CONFIG = {
    'exp_time': 36000,  # å®éªŒè¶…æ—¶æ—¶é—´(ç§’)
    'round_time': 600,  # æ¯è½®è¶…æ—¶æ—¶é—´(ç§’)
    'request_model_name': 'default-model',  # é»˜è®¤æ¨¡å‹åç§°
    'vllm_engine': None,  # vLLMå¼•æ“å®ä¾‹
}

# ç»“æœæ–‡ä»¶è·¯å¾„
RESULTS_FILE = "../results/benchmark_results.json"
EOF
```

### 2. åˆ›å»ºrequirements.txt
```bash
cat > requirements.txt << 'EOF'
# vLLMå’Œç›¸å…³ä¾èµ–
vllm>=0.2.0
torch>=2.0.0
transformers>=4.30.0
accelerate>=0.20.0

# æ•°æ®å¤„ç†
numpy>=1.21.0
pandas>=1.5.0
datasets>=2.0.0

# ç½‘ç»œè¯·æ±‚
requests>=2.28.0
aiohttp>=3.8.0
openai>=0.27.0

# å¯è§†åŒ–
matplotlib>=3.5.0
seaborn>=0.11.0

# å·¥å…·åº“
tqdm>=4.64.0
psutil>=5.9.0
pyyaml>=6.0
EOF
```

## ğŸƒâ€â™‚ï¸ è¿è¡ŒåŸºå‡†æµ‹è¯•

### 1. è®¾ç½®è„šæœ¬æƒé™
```bash
chmod +x start_vllm_benchmark.sh
```

### 2. é…ç½®æ¨¡å‹è·¯å¾„
```bash
# ç¼–è¾‘å¯åŠ¨è„šæœ¬ï¼Œä¿®æ”¹æ¨¡å‹è·¯å¾„
vim start_vllm_benchmark.sh

# ä¿®æ”¹ä»¥ä¸‹è¡Œï¼š
# MODEL_PATH="/path/to/your/model"  # æ”¹ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
# TOKENIZER_PATH="/path/to/your/tokenizer"  # æ”¹ä¸ºä½ çš„å®é™…tokenizerè·¯å¾„
```

### 3. è¿è¡ŒåŸºå‡†æµ‹è¯•
```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source vllm_env/bin/activate

# è¿è¡ŒåŸºå‡†æµ‹è¯•
./start_vllm_benchmark.sh
```

## ğŸ› å¸¸è§é—®é¢˜è§£å†³

### 1. CUDAç›¸å…³é—®é¢˜
```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version

# å¦‚æœCUDAä¸å¯ç”¨ï¼Œé‡æ–°å®‰è£…é©±åŠ¨
sudo apt purge nvidia-*
sudo apt install nvidia-driver-535  # æˆ–æœ€æ–°ç‰ˆæœ¬
sudo reboot
```

### 2. å†…å­˜ä¸è¶³é—®é¢˜
```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h

# å¢åŠ äº¤æ¢ç©ºé—´
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

### 3. PythonåŒ…å†²çª
```bash
# æ¸…ç†pipç¼“å­˜
pip cache purge

# é‡æ–°åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
deactivate
rm -rf vllm_env
python3 -m venv vllm_env
source vllm_env/bin/activate
pip install --upgrade pip
```

### 4. ç«¯å£å ç”¨é—®é¢˜
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tulpn | grep :8000

# æ€æ­»å ç”¨ç«¯å£çš„è¿›ç¨‹
sudo kill -9 <PID>
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç³»ç»Ÿä¼˜åŒ–
```bash
# è®¾ç½®GPUæ€§èƒ½æ¨¡å¼
sudo nvidia-smi -pm 1
sudo nvidia-smi -pl 300  # è®¾ç½®åŠŸç‡é™åˆ¶

# ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½
echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
```

### 2. vLLMé…ç½®ä¼˜åŒ–
```bash
# åœ¨å¯åŠ¨è„šæœ¬ä¸­è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š
# - tensor_parallel_size: æ ¹æ®GPUæ•°é‡è®¾ç½®
# - max_num_seqs: æ ¹æ®GPUå†…å­˜è°ƒæ•´
# - gpu_memory_utilization: å»ºè®®0.8-0.9
```

### 3. ç›‘æ§å·¥å…·
```bash
# å®‰è£…ç›‘æ§å·¥å…·
pip install gpustat
sudo apt install iotop

# å®æ—¶ç›‘æ§GPU
watch -n 1 gpustat

# ç›‘æ§ç³»ç»Ÿèµ„æº
htop
```

## ğŸ”„ è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬

åˆ›å»ºä¸€é”®éƒ¨ç½²è„šæœ¬ï¼š

```bash
cat > setup_ubuntu.sh << 'EOF'
#!/bin/bash
set -e

echo "ğŸš€ å¼€å§‹Ubuntuç¯å¢ƒè®¾ç½®..."

# æ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y

# å®‰è£…åŸºç¡€ä¾èµ–
sudo apt install -y python3.9 python3.9-dev python3.9-venv python3-pip build-essential

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.9 -m venv vllm_env
source vllm_env/bin/activate

# å®‰è£…Pythonä¾èµ–
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install vllm transformers accelerate

# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p log models results prompt_hub config

echo "âœ… ç¯å¢ƒè®¾ç½®å®Œæˆï¼"
echo "è¯·è¿è¡Œ: source vllm_env/bin/activate æ¿€æ´»ç¯å¢ƒ"
EOF

chmod +x setup_ubuntu.sh
```

## ğŸ“ ä½¿ç”¨è¯´æ˜

1. **é¦–æ¬¡è®¾ç½®**: è¿è¡Œ `./setup_ubuntu.sh` è¿›è¡Œä¸€é”®ç¯å¢ƒé…ç½®
2. **æ—¥å¸¸ä½¿ç”¨**: 
   ```bash
   source vllm_env/bin/activate
   ./start_vllm_benchmark.sh
   ```
3. **æŸ¥çœ‹ç»“æœ**: ç»“æœä¿å­˜åœ¨ `results/` ç›®å½•ä¸‹
4. **æ—¥å¿—æŸ¥çœ‹**: æ—¥å¿—ä¿å­˜åœ¨ `log/` ç›®å½•ä¸‹

ç°åœ¨ä½ å¯ä»¥åœ¨Ubuntuä¸Šé¡ºåˆ©ç¼–è¯‘å’Œè¿è¡ŒvLLMåŸºå‡†æµ‹è¯•äº†ï¼ğŸ‰ 