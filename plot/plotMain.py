import json
import math
import os
import matplotlib.pyplot as plt
import numpy as np

from config.Config import GLOBAL_CONFIG
from plot.plotUtil import xLabel_time, plot_metrics_with_annotations, setup_subplot, setup_subplot_client, \
    plot_fairness_index

line_styles = ['-', '--', '-.', ':']
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
markers = ['o', 's', '^', 'D', 'v', '*']


def plot_averaged_results(short_results, long_results, args_concurrency, total_time, filename, exp_type):
    # åˆ›å»ºä¸€ä¸ª4Ã—1çš„å­å›¾å¸ƒå±€
    fig2, axs2 = plt.subplots(4, 1, figsize=(14, 20))

    # æ·»åŠ æ ‡é¢˜æ˜¾ç¤ºå‚æ•°
    fig2.suptitle(
        f"{exp_type} System Benchmark Results - Short Client: {args_concurrency}, Total Time: {total_time}, Alpha: {GLOBAL_CONFIG.get('alpha', 0.5)}",
        fontsize=16, y=0.98)

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_results = short_results + long_results

    if not all_results:
        return

    avg_success_rate = []
    avg_tokens_per_second = []
    avg_latency = []
    avg_ttft = []
    avg_rps = []
    avg_tokens_count = []
    avg_total_output_tokens = []
    avg_total_input_tokens = []
    concurrency = []
    qps = []
    time = []

    # Get the shortest length among all results
    min_len = min(len(result) for result in all_results)

    for i in range(min_len):
        time.append(all_results[0][i]["time"])
        # æ€»QPSæ˜¯æ‰€æœ‰å®¢æˆ·ç«¯QPSä¹‹å’Œ
        qps.append(all_results[0][i]["qps"] * len(all_results))
        concurrency.append(all_results[0][i]["concurrency"])

        # è®¡ç®—æ€»å’Œè€Œä¸æ˜¯å¹³å‡å€¼
        tokens_count = sum(result[i]['total_output_tokens'] + result[i]['total_input_tokens'] for result in all_results)
        total_output_tokens = sum(result[i]['total_output_tokens'] for result in all_results)
        total_input_tokens = sum(result[i]['total_input_tokens'] for result in all_results)
        # æˆåŠŸè¯·æ±‚æ•°æ€»å’Œ
        successful_requests = sum(result[i]['successful_requests'] for result in all_results)
        total_requests = sum(result[i]['total_requests'] for result in all_results)
        success_rate = successful_requests * 100 / total_requests if total_requests > 0 else 0
        # å–æ‰€æœ‰å®¢æˆ·ç«¯å»¶è¿Ÿçš„å’Œä½œä¸ºç³»ç»Ÿå»¶è¿Ÿ
        tokens_per_second = sum(result[i]["tokens_per_second"]["p99"] for result in all_results)
        latency = max(result[i]["latency"]["p99"] for result in all_results)
        ttft = max(result[i]["time_to_first_token"]["p99"] for result in all_results)
        # æ€»RPSæ˜¯æ‰€æœ‰å®¢æˆ·ç«¯RPSä¹‹å’Œ
        rps = sum(result[i]["requests_per_second"] for result in all_results)

        avg_tokens_count.append(tokens_count)
        avg_total_output_tokens.append(total_output_tokens)
        avg_total_input_tokens.append(total_input_tokens)
        avg_success_rate.append(success_rate)
        avg_tokens_per_second.append(tokens_per_second)
        avg_latency.append(latency)
        avg_ttft.append(ttft)
        avg_rps.append(rps)

    time_xLabel = xLabel_time(time)

    # Plot tokens count with different line styles
    plot_metrics_with_annotations(axs2[0], range(len(qps)), avg_tokens_count, 'tokens count',
                                  colors[0], markers[0], linestyle=line_styles[0])
    plot_metrics_with_annotations(axs2[0], range(len(qps)), avg_total_output_tokens, 'total_output_tokens',
                                  colors[1], markers[1], linestyle=line_styles[1])
    plot_metrics_with_annotations(axs2[0], range(len(qps)), avg_total_input_tokens, 'total_input_tokens',
                                  colors[2], markers[2], linestyle=line_styles[2])
    setup_subplot(axs2[0],
                  "Sum Input and Output Tokens",
                  time_xLabel,
                  plt.FuncFormatter(lambda x, p: format(int(x), ',')))

    # Plot success rate
    plot_metrics_with_annotations(axs2[1], range(len(qps)), avg_success_rate, 'success_rate',
                                  colors[3], markers[3], linestyle=line_styles[0])
    setup_subplot(axs2[1],
                  "Average Success Rate",
                  time_xLabel,
                  ylim=(max(min(avg_success_rate) - 10, 0), 105))

    # Plot tokens per second
    plot_metrics_with_annotations(axs2[2], range(len(qps)), avg_tokens_per_second, 'tokens_per_second',
                                  colors[4], markers[4], xytext=(0, -15), linestyle=line_styles[0])
    setup_subplot(axs2[2],
                  "Sum Tokens/s",
                  time_xLabel)

    # Plot latency metrics
    plot_metrics_with_annotations(axs2[3], range(len(qps)), avg_latency, 'latency',
                                  colors[0], markers[0], linestyle=line_styles[0])
    plot_metrics_with_annotations(axs2[3], range(len(qps)), avg_ttft, 'time_to_first_token',
                                  colors[1], markers[1], xytext=(0, -15), linestyle=line_styles[1])
    plot_metrics_with_annotations(axs2[3], range(len(qps)), avg_rps, 'requests_per_second',
                                  colors[2], markers[2], xytext=(15, 0), linestyle=line_styles[2])
    setup_subplot(axs2[3],
                  "Max Latency and TTFT, Sum RPS Metrics",
                  time_xLabel)

    # ä¼˜åŒ–å¸ƒå±€
    fig2.tight_layout(pad=3.0, h_pad=2.0)
    # è°ƒæ•´å­å›¾é—´è·ï¼Œä¸ºåº•éƒ¨å›¾ä¾‹ç•™å‡ºç©ºé—´
    fig2.subplots_adjust(top=0.92, bottom=0.1)

    # åˆ›å»º figure æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists('figure'):
        os.makedirs('figure')

    # ä¿å­˜å›¾ç‰‡
    fig2.savefig('figure/system_results' + filename.split('.')[0] + '.png', dpi=300, bbox_inches='tight')
    plt.show()
    return time_xLabel


def plot_comprehensive_results(sorted_all_results, args_concurrency, total_time, filename, exp_type, qps_with_time):
    """
    ç»˜åˆ¶ç»¼åˆæ€§èƒ½å›¾è¡¨ï¼ŒåªåŒ…æ‹¬å„å®¢æˆ·ç«¯çš„æ€§èƒ½æŒ‡æ ‡
    """
    # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡å›¾
    fig1, axs1 = plt.subplots(4, 2, figsize=(28, 25))  # æ€§èƒ½æŒ‡æ ‡å›¾
    axs1 = axs1.flatten()  # å°†2Dæ•°ç»„å±•å¹³ä¸º1Dï¼Œä¾¿äºç´¢å¼•

    # æ·»åŠ æ ‡é¢˜æ˜¾ç¤ºå‚æ•°
    fig1.suptitle(
        f"{exp_type} Performance Metrics - Concurrency: {args_concurrency}, Total Time: {total_time}, Alpha: {GLOBAL_CONFIG.get('alpha', 0.5)}",
        fontsize=16, y=0.98)

    # è·å–æ‰€æœ‰å®¢æˆ·ç«¯
    clients = set()
    for result in sorted_all_results:
        clients.add(f'{result[0]["client_index"]}_slo_{result[0]["latency_slo"]}')

    # å°†clientsåˆ†ä¸ºshortå’Œlongä¸¤ç»„
    short_clients = sorted([c for c in clients if "short" in c])
    long_clients = sorted([c for c in clients if "long" in c])

    print(f"Short clients: {short_clients}")
    print(f"Long clients: {long_clients}")

    # æš–è‰²ç³»ç”¨äºshort clients
    warm_colors = ['#FF4D4D', '#FFA64D', '#FFD700', '#FF69B4', '#FF8C69']
    # å†·è‰²ç³»ç”¨äºlong clients  
    cool_colors = ['#4169E1', '#00CED1', '#6495ED', '#483D8B', '#008B8B']

    # åˆ›å»ºä¸€ä¸ªå…±äº«å›¾ä¾‹çš„å¥æŸ„å’Œæ ‡ç­¾åˆ—è¡¨
    legend_handles = []
    legend_labels = []

    # ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡
    # 1. æˆåŠŸç‡
    plot_client_metric(axs1[0], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "success_rate",
                       "Success Rate (%)", legend_handles, legend_labels)

    # 2. æ¯ç§’ä»¤ç‰Œæ•°
    plot_client_metric(axs1[1], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "tokens_per_second",
                       "Tokens/s", legend_handles, legend_labels)

    # 3. å»¶è¿Ÿ
    plot_client_metric(axs1[2], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "latency",
                       "Latency (ms)", legend_handles, legend_labels)

    # 4. é¦–ä¸ªä»¤ç‰Œæ—¶é—´
    plot_client_metric(axs1[3], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "time_to_first_token",
                       "Time to First Token (ms)", legend_handles, legend_labels)

    # 5. æ¯ç§’è¯·æ±‚æ•°
    plot_client_metric(axs1[4], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "requests_per_second",
                       "Requests per Second", legend_handles, legend_labels)

    # 6. æ¯ä¸ªå®¢æˆ·ç«¯çš„æœåŠ¡ä½¿ç”¨é‡
    plot_client_metric(axs1[5], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "service",
                       "Service Usage", legend_handles, legend_labels)

    # 7. æˆåŠŸè¯·æ±‚æ•°
    plot_client_metric(axs1[6], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "successful_requests",
                       "Successful Requests", legend_handles, legend_labels)

    # 8. è¿åSLOçš„è¯·æ±‚æ•°é‡
    plot_client_metric(axs1[7], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "slo_violation_count",
                       "Slo Violation Request Numbers", legend_handles, legend_labels)

    # åœ¨æ€§èƒ½æŒ‡æ ‡å›¾åº•éƒ¨æ·»åŠ å…±äº«å›¾ä¾‹
    fig1.legend(handles=legend_handles, labels=legend_labels,
               loc='center', bbox_to_anchor=(0.5, 0.02),
               ncol=len(short_clients) + len(long_clients),
               fontsize=10)

    # ä¼˜åŒ–å¸ƒå±€
    fig1.tight_layout(pad=3.0, h_pad=2.0, w_pad=2.0)
    # è°ƒæ•´å­å›¾é—´è·ï¼Œä¸ºåº•éƒ¨å›¾ä¾‹ç•™å‡ºç©ºé—´
    fig1.subplots_adjust(top=0.92, bottom=0.1)

    # åˆ›å»ºfigureæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists('figure'):
        os.makedirs('figure')

    # ä¿å­˜å›¾è¡¨
    fig1.savefig(f'figure/performance_metrics{filename.split(".")[0]}.png', dpi=300, bbox_inches='tight')

    plt.show()


def plot_fairness_results(sorted_all_results, args_concurrency, total_time, filename, exp_type, fairness_results):
    """
    ç»˜åˆ¶å…¬å¹³æ€§ç›¸å…³å›¾è¡¨
    """
    # åˆ›å»ºå…¬å¹³æ€§æŒ‡æ ‡å›¾
    fig2, axs2 = plt.subplots(3, 1, figsize=(28, 18))   # å…¬å¹³æ€§æŒ‡æ ‡å›¾ï¼Œ3è¡Œ1åˆ—
    axs2 = axs2.flatten()

    # æ·»åŠ æ ‡é¢˜æ˜¾ç¤ºå‚æ•°
    fig2.suptitle(
        f"{exp_type} Fairness Metrics - Concurrency: {args_concurrency}, Total Time: {total_time}, Alpha: {GLOBAL_CONFIG.get('alpha', 0.5)}",
        fontsize=16, y=0.98)

    # è·å–æ‰€æœ‰å®¢æˆ·ç«¯
    clients = set()
    for result in sorted_all_results:
        clients.add(f'{result[0]["client_index"]}_slo_{result[0]["latency_slo"]}')

    # å°†clientsåˆ†ä¸ºshortå’Œlongä¸¤ç»„
    short_clients = sorted([c for c in clients if "short" in c])
    long_clients = sorted([c for c in clients if "long" in c])

    # æš–è‰²ç³»ç”¨äºshort clients
    warm_colors = ['#FF4D4D', '#FFA64D', '#FFD700', '#FF69B4', '#FF8C69']
    # å†·è‰²ç³»ç”¨äºlong clients  
    cool_colors = ['#4169E1', '#00CED1', '#6495ED', '#483D8B', '#008B8B']

    # åˆ›å»ºä¸€ä¸ªå…±äº«å›¾ä¾‹çš„å¥æŸ„å’Œæ ‡ç­¾åˆ—è¡¨
    legend_handles = []
    legend_labels = []

    # ç»˜åˆ¶å…¬å¹³æ€§æŒ‡æ ‡
    # 1. Fairness Ratio
    plot_client_metric(axs2[0], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "fairness_ratio",
                       "Fairness Ratio", legend_handles, legend_labels)

    # 2. Jain'så…¬å¹³æ€§æŒ‡æ•°
    if fairness_results:
        f_values = [result['f_result'] for result in fairness_results]
        times = list(range(len(f_values)))
        if len(times) == len(f_values):
            plot_fairness_index(axs2[1], f_values, times)
        else:
            print(f"Warning: Mismatched lengths - times: {len(times)}, f_values: {len(f_values)}")
            min_len = min(len(times), len(f_values))
            plot_fairness_index(axs2[1], f_values[:min_len], times[:min_len])
    else:
        # å¦‚æœæ²¡æœ‰fairnessæ•°æ®ï¼Œæ˜¾ç¤ºç©ºå›¾è¡¨
        axs2[1].text(0.5, 0.5, 'No Fairness Data Available', 
                     horizontalalignment='center', verticalalignment='center',
                     transform=axs2[1].transAxes, fontsize=12)
        axs2[1].set_title("Jain's Fairness Index")
        axs2[1].set_xlabel("Time")
        axs2[1].set_ylabel("Fairness Index")
    
    # 3. creditå€¼
    plot_client_metric(axs2[2], sorted_all_results, short_clients, long_clients,
                       warm_colors, cool_colors, "credit",
                       "Clients Credit", legend_handles, legend_labels, ylim=None)

    # ä¸ºç¬¬ä¸‰å¼ å›¾æ·»åŠ å›¾ä¾‹
    axs2[2].legend()

    # ä¼˜åŒ–å¸ƒå±€
    fig2.tight_layout(pad=3.0, h_pad=2.0, w_pad=2.0)
    # è°ƒæ•´å­å›¾é—´è·ï¼Œä¸ºåº•éƒ¨å›¾ä¾‹ç•™å‡ºç©ºé—´
    fig2.subplots_adjust(top=0.92, bottom=0.1)

    # åˆ›å»ºfigureæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists('figure'):
        os.makedirs('figure')

    # ä¿å­˜å›¾è¡¨
    fig2.savefig(f'figure/fairness_metrics{filename.split(".")[0]}.png', dpi=300, bbox_inches='tight')

    plt.show()


def plot_client_metric(ax, sorted_all_results, short_clients, long_clients, warm_colors, cool_colors, metric_key, title,
                       legend_handles, legend_labels, ylim=None):
    plotted_value_sets = []
    """ç»˜åˆ¶å®¢æˆ·ç«¯æŒ‡æ ‡"""
    # å…ˆç»˜åˆ¶short clientsï¼Œä½¿ç”¨å®çº¿
    for i, client in enumerate(short_clients):
        client_results = next(
            r for r in sorted_all_results if f'{r[0]["client_index"]}_slo_{r[0]["latency_slo"]}' == client)
        time = [result['time'] for result in client_results]
        time_xLabel = xLabel_time(time)

        if metric_key == "success_rate":
            values = [result['successful_requests'] * 100 / result['total_requests'] for result in client_results]
        elif metric_key in ["tokens_per_second", "latency", "time_to_first_token"]:
            values = [result[metric_key]["p99"] for result in client_results]
        elif metric_key == "service":
            values = [result['total_input_tokens'] + result['total_output_tokens'] * 2 for result in client_results]
        else:
            values = [result[metric_key] for result in client_results]

        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒé•¿åº¦å’Œç›¸åŒå€¼çš„æ•°ç»„ï¼Œé¿å…å½¢çŠ¶ä¸åŒ¹é…çš„é”™è¯¯
        is_duplicate = any(len(values) == len(prev) and np.allclose(values, prev, atol=1e-4) for prev in plotted_value_sets)
        if is_duplicate:
            x_vals = [j + 0.02 for j in range(len(time))]  # åç§»ä¸€ç‚¹
        else:
            x_vals = list(range(len(time)))

        # ä½¿ç”¨ä¸åŒçš„markerå’Œlinestyleæ¥åŒºåˆ†ä¸åŒçš„çº¿æ¡
        line = ax.plot(x_vals, values, marker=markers[i % len(markers)],
                       color=warm_colors[i % len(warm_colors)], linestyle='-',
                       label=client, alpha=0.7, markersize=8)[0]

        # ğŸ‘‡ ä¿å­˜å·²ç”»è¿‡çš„ values
        plotted_value_sets.append(values)

        if client not in legend_labels:
            legend_handles.append(line)
            legend_labels.append(client)

    # å†ç»˜åˆ¶long clientsï¼Œä½¿ç”¨è™šçº¿
    for i, client in enumerate(long_clients):
        client_results = next(
            r for r in sorted_all_results if f'{r[0]["client_index"]}_slo_{r[0]["latency_slo"]}' == client)
        time = [result['time'] for result in client_results]
        time_xLabel = xLabel_time(time)

        if metric_key == "success_rate":
            values = [result['successful_requests'] * 100 / result['total_requests'] for result in client_results]
        elif metric_key in ["tokens_per_second", "latency", "time_to_first_token"]:
            values = [result[metric_key]["p99"] for result in client_results]
        elif metric_key == "service":
            values = [result['total_input_tokens'] + result['total_output_tokens'] * 2 for result in client_results]
        else:
            values = [result[metric_key] for result in client_results]

        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒé•¿åº¦å’Œç›¸åŒå€¼çš„æ•°ç»„ï¼Œé¿å…å½¢çŠ¶ä¸åŒ¹é…çš„é”™è¯¯
        is_duplicate = any(len(values) == len(prev) and np.allclose(values, prev, atol=1e-4) for prev in plotted_value_sets)
        if is_duplicate:
            x_vals = [j + 0.02 for j in range(len(time))]  # åç§»ä¸€ç‚¹
        else:
            x_vals = list(range(len(time)))

        # ä½¿ç”¨ä¸åŒçš„markerå’Œlinestyleæ¥åŒºåˆ†ä¸åŒçš„çº¿æ¡
        line = ax.plot(x_vals, values, marker=markers[(i + len(short_clients)) % len(markers)],
                       color=cool_colors[i % len(cool_colors)], linestyle='--',
                       label=client, alpha=0.7, markersize=8)[0]

        plotted_value_sets.append(values)

        if client not in legend_labels:
            legend_handles.append(line)
            legend_labels.append(client)

    # è®¡ç®—æ‰€æœ‰å€¼ä»¥ç¡®å®šåˆé€‚çš„yè½´èŒƒå›´
    all_values = []
    for client in short_clients + long_clients:
        try:
            client_results = next(
                r for r in sorted_all_results if f'{r[0]["client_index"]}_slo_{r[0]["latency_slo"]}' == client)
            if metric_key == "success_rate":
                values = [result['successful_requests'] * 100 / result['total_requests'] for result in client_results]
            elif metric_key in ["tokens_per_second", "latency", "time_to_first_token"]:
                values = [result[metric_key]["p99"] for result in client_results]
            elif metric_key == "service":
                values = [result['total_input_tokens'] + result['total_output_tokens'] * 2 for result in client_results]
            else:
                values = [result[metric_key] for result in client_results]
            all_values.extend(values)
        except (StopIteration, KeyError, ZeroDivisionError) as e:
            print(f"Warning: Error processing {client} for {metric_key}: {e}")

    print(f"[DEBUG] {metric_key} all_values: {all_values}")

    # å¦‚æœæ²¡æœ‰æä¾›ylimä¸”æœ‰è¶³å¤Ÿçš„å€¼ï¼Œåˆ™è‡ªåŠ¨è®¡ç®—åˆé€‚çš„èŒƒå›´
    if ylim is None and all_values:
        try:
            # è¿‡æ»¤æ‰Noneå€¼
            valid_values = [v for v in all_values if v is not None]
            if valid_values:  # ç¡®ä¿æœ‰æœ‰æ•ˆå€¼
                min_val = min(valid_values)
                max_val = max(valid_values)
                # å¢åŠ æœ€å°marginï¼Œé¿å…yè½´èŒƒå›´è¿‡å°
                margin = max((max_val - min_val) * 0.1, 1.0)
                if min_val < 0:
                    computed_ylim = (min_val - margin, max_val + margin)
                else:
                    computed_ylim = (max(0, min_val - margin), max_val + margin)
                # ç‰¹æ®Šå¤„ç† success_rate
                if metric_key == "success_rate":
                    computed_ylim = (max(0, min_val - 5), 105)
            else:
                computed_ylim = None  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå€¼ï¼Œè®¾ç½®ä¸ºNone
        except (ValueError, TypeError) as e:
            print(f"Warning: Error computing ylim for {metric_key}: {e}")
            computed_ylim = None
    else:
        computed_ylim = ylim

    # è®¾ç½®å­å›¾å±æ€§
    setup_subplot_client(ax, title, time_xLabel, ylim=computed_ylim)


def plot_aggregated_results(sorted_all_results, args_concurrency, total_time, filename, exp_type):
    """
    ç»˜åˆ¶æ‰€æœ‰å®¢æˆ·ç«¯çš„æ±‡æ€»å›¾è¡¨ï¼ˆæ€»å’Œæˆ–å¹³å‡å€¼ï¼‰
    """
    # åˆ›å»ºæ±‡æ€»å›¾è¡¨
    fig, axs = plt.subplots(4, 2, figsize=(28, 25))
    axs = axs.flatten()

    # æ·»åŠ æ ‡é¢˜
    fig.suptitle(
        f"{exp_type} Aggregated System Metrics - Concurrency: {args_concurrency}, Total Time: {total_time}, Alpha: {GLOBAL_CONFIG.get('alpha', 0.5)}",
        fontsize=16, y=0.98)

    if not sorted_all_results:
        return

    # è·å–æ—¶é—´è½´ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„æ—¶é—´ï¼‰
    time_data = [result['time'] for result in sorted_all_results[0]]
    time_xLabel = xLabel_time(time_data)
    x_values = list(range(len(time_data)))

    # å®šä¹‰æŒ‡æ ‡å’Œå…¶æ±‡æ€»æ–¹å¼
    metrics_config = [
        {
            'key': 'success_rate',
            'title': 'Average Success Rate (%)',
            'method': 'average',
            'extractor': lambda result: result['successful_requests'] * 100 / result['total_requests'] if result['total_requests'] > 0 else 0
        },
        {
            'key': 'tokens_per_second',
            'title': 'Total Tokens/s',
            'method': 'sum',
            'extractor': lambda result: result['tokens_per_second']['p99']
        },
        {
            'key': 'latency',
            'title': 'Average Latency (ms)',
            'method': 'average',
            'extractor': lambda result: result['latency']['p99']
        },
        {
            'key': 'time_to_first_token',
            'title': 'Average Time to First Token (ms)',
            'method': 'average',
            'extractor': lambda result: result['time_to_first_token']['p99']
        },
        {
            'key': 'requests_per_second',
            'title': 'Total Requests per Second',
            'method': 'sum',
            'extractor': lambda result: result['requests_per_second']
        },
        {
            'key': 'service',
            'title': 'Total Service Usage',
            'method': 'sum',
            'extractor': lambda result: result['total_input_tokens'] + result['total_output_tokens'] * 2
        },
        {
            'key': 'successful_requests',
            'title': 'Total Successful Requests',
            'method': 'sum',
            'extractor': lambda result: result['successful_requests']
        },
        {
            'key': 'slo_violation_count',
            'title': 'Total SLO Violation Count',
            'method': 'sum',
            'extractor': lambda result: result['slo_violation_count']
        }
    ]

    # ä¸ºæ¯ä¸ªæŒ‡æ ‡è®¡ç®—æ±‡æ€»å€¼å¹¶ç»˜å›¾
    for idx, metric_config in enumerate(metrics_config):
        aggregated_values = []
        
        # å¯¹æ¯ä¸ªæ—¶é—´ç‚¹è®¡ç®—æ±‡æ€»å€¼
        for time_idx in range(len(time_data)):
            time_values = []
            
            # æ”¶é›†æ‰€æœ‰å®¢æˆ·ç«¯åœ¨è¯¥æ—¶é—´ç‚¹çš„å€¼
            for client_results in sorted_all_results:
                if time_idx < len(client_results):
                    try:
                        value = metric_config['extractor'](client_results[time_idx])
                        time_values.append(value)
                    except (KeyError, ZeroDivisionError, TypeError) as e:
                        print(f"Warning: Error extracting {metric_config['key']} at time {time_idx}: {e}")
                        continue
            
            # æ ¹æ®æ–¹æ³•è®¡ç®—æ±‡æ€»å€¼
            if time_values:
                if metric_config['method'] == 'sum':
                    aggregated_value = sum(time_values)
                elif metric_config['method'] == 'average':
                    aggregated_value = sum(time_values) / len(time_values)
                else:
                    aggregated_value = sum(time_values)  # é»˜è®¤ä½¿ç”¨æ€»å’Œ
                aggregated_values.append(aggregated_value)
            else:
                aggregated_values.append(0)

        # ç»˜åˆ¶å›¾è¡¨
        color = colors[idx % len(colors)]
        marker = markers[idx % len(markers)]
        linestyle = line_styles[idx % len(line_styles)]
        
        axs[idx].plot(x_values, aggregated_values, 
                     marker=marker, color=color, linestyle=linestyle,
                     linewidth=2, markersize=6, alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨ï¼ˆæ¯éš”å‡ ä¸ªç‚¹æ ‡æ³¨ä¸€æ¬¡ï¼Œé¿å…è¿‡äºæ‹¥æŒ¤ï¼‰
        annotation_step = max(1, len(aggregated_values) // 8)
        for i in range(0, len(aggregated_values), annotation_step):
            axs[idx].annotate(f'{aggregated_values[i]:.1f}',
                             (x_values[i], aggregated_values[i]),
                             xytext=(0, 10), textcoords='offset points',
                             ha='center', va='bottom', fontsize=8,
                             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))

        # è®¾ç½®å­å›¾å±æ€§
        axs[idx].set_title(metric_config['title'], fontsize=14, fontweight='bold')
        axs[idx].set_xlabel('Time', fontsize=12)
        axs[idx].set_ylabel(metric_config['title'].split()[-1] if len(metric_config['title'].split()) > 1 else 'Value', fontsize=12)
        axs[idx].grid(True, alpha=0.3)
        axs[idx].set_xticks(range(0, len(time_xLabel), max(1, len(time_xLabel) // 10)))
        axs[idx].set_xticklabels([time_xLabel[i] for i in range(0, len(time_xLabel), max(1, len(time_xLabel) // 10))], 
                                rotation=45, ha='right')

        # è®¾ç½®yè½´èŒƒå›´
        if aggregated_values:
            min_val = min(aggregated_values)
            max_val = max(aggregated_values)
            margin = (max_val - min_val) * 0.1 if max_val != min_val else 1
            
            if metric_config['key'] == 'success_rate':
                axs[idx].set_ylim(max(0, min_val - 5), 105)
            elif min_val >= 0:
                axs[idx].set_ylim(max(0, min_val - margin), max_val + margin)
            else:
                axs[idx].set_ylim(min_val - margin, max_val + margin)

        print(f"[DEBUG] {metric_config['key']} aggregated values: {aggregated_values[:5]}...{aggregated_values[-5:] if len(aggregated_values) > 5 else ''}")

    # ä¼˜åŒ–å¸ƒå±€
    fig.tight_layout(pad=3.0, h_pad=2.0, w_pad=2.0)
    fig.subplots_adjust(top=0.92, bottom=0.1)

    # åˆ›å»ºfigureæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists('figure'):
        os.makedirs('figure')

    # ä¿å­˜å›¾è¡¨
    fig.savefig(f'figure/aggregated_metrics{filename.split(".")[0]}.png', dpi=300, bbox_inches='tight')

    plt.show()


def plot_result(plot_data):
    with open("results/" + plot_data["filename"], 'r') as f:
        all_results = json.load(f)

    # Load fairness results and create third figure
    fairness_file_path = f"tmp_result/tmp_fairness_result_{GLOBAL_CONFIG.get('monitor_file_time')}.json"
    try:
        with open(fairness_file_path, 'r') as f:
            fairness_results = json.load(f)
    except FileNotFoundError:
        print(f"Warning: Fairness results file not found at {fairness_file_path}")
        print("Continuing without fairness data...")
        fairness_results = []

    if len(all_results) >= 1:
        # Group results by short/long and sort by client_index
        short_results = []
        long_results = []

        for result in all_results:
            if "short" in result[0]["client_index"]:
                short_results.append(result)
            else:
                long_results.append(result)

        # Sort results by client_index
        short_results.sort(key=lambda x: x[0]["client_index"])
        long_results.sort(key=lambda x: x[0]["client_index"])

        # Combine sorted results with short first, then long
        sorted_all_results = short_results + long_results

        qps_with_time = plot_averaged_results(short_results, long_results, plot_data["concurrency"],
                                              plot_data["total_time"], plot_data["filename"],
                                              plot_data["exp"])
        plot_comprehensive_results(sorted_all_results, plot_data["concurrency"], plot_data["total_time"],
                                   plot_data["filename"], plot_data["exp"], qps_with_time)
        plot_fairness_results(sorted_all_results, plot_data["concurrency"], plot_data["total_time"],
                               plot_data["filename"], plot_data["exp"], fairness_results)
        plot_aggregated_results(sorted_all_results, plot_data["concurrency"], plot_data["total_time"],
                               plot_data["filename"], plot_data["exp"])
    else:
        print("No results found")
        return


if __name__ == "__main__":
    with open("tmp_result/plot_data.json", "r") as f:
        data = json.load(f)

    plot_result(data)
